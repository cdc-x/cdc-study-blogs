# ElasticSearch简单查询

## ES 的两种查询方式

### **一、字符串查询**

#### **1.1 按文档ID查询**

```python
# 查询木叶村中文档ID是2的忍者信息
GET muye/class/2
```

查询结果：

```python
{
  "_index" : "muye",
  "_type" : "class",
  "_id" : "2",
  "_version" : 6,
  "found" : true,
  "_source" : {
    "class_name" : "卡卡西班",
    "name" : "漩涡鸣人",
    "gender" : "male",
    "age" : 16,
    "skill" : [
      "嘴遁",
      "螺旋丸",
      "影分身",
      "仙人模式"
    ]
  }
}
```

#### **1.2	按条件查询**

```python
# 查询年龄是16岁的忍者信息
GET muye/class/_search?q=age:16
```

还是使用 GET 命令，通过 \_serarch 查询，查询条件是 age 属性是 16 的人都有哪些。_search 后的 ?q= 后面跟的就是查询条件。

查询结果:

```python
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "muye",
        "_type" : "class",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "class_name" : "卡卡西班",
          "name" : "漩涡鸣人",
          "gender" : "male",
          "age" : 16,
          "skill" : [
            "嘴遁",
            "螺旋丸",
            "影分身",
            "仙人模式"
          ]
        }
      },
      {
        "_index" : "muye",
        "_type" : "class",
        "_id" : "3",
        "_score" : 1.0,
        "_source" : {
          "class_name" : "卡卡西班",
          "name" : "宇智波佐助",
          "gender" : "male",
          "age" : 16,
          "skill" : [
            "写轮眼",
            "千鸟",
            "装遁"
          ]
        }
      }
    ]
  }
}
```

`hits` 是返回的结果集，所有符合查询条件的文档都会包裹在里面。

### **二、结构化查询**

```python
# 查询木叶村所有的忍者信息

GET muye/class/_search
{
  "query": {
    "match_all": {}
  }
}
```

DSL（结构化）查询方式在 ES 中应用的比较多，我们后面针对 ES 查询方式的研究主要也是针对 DSL 查询方式的。

## match 查询和 term 查询

### **一、match 查询**

#### **1.1 match_all 查询所有**

```python
# 查询木叶村所有的忍者信息
GET muye/class/_search
{
  "query": {
    "match_all": {}
  }
}
```

#### **1.2 match 条件查询**

```python
# 查询名字是旗木卡卡西的忍者信息
GET muye/class/_search
{
  "query": {
    "match": {
      "name": "旗木卡卡西"
    }
  }
}
```

查询结果

```python
{
  "took" : 95,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.654172,
    "hits" : [
      {
        "_index" : "muye",
        "_type" : "class",
        "_id" : "1",
        "_score" : 1.654172,
        "_source" : {
          "class_name" : "卡卡西班",
          "name" : "旗木卡卡西",
          "gender" : "male",
          "age" : 26,
          "skill" : [
            "写轮眼",
            "雷切"
          ]
        }
      }
    ]
  }
}
```

#### **1.3 match_phrase 短语查询**

我们先来造点数据

```python
PUT t1/doc/1
{
  "title": "中国是世界上人口最多的国家"
}

PUT t1/doc/2
{
  "title": "美国是世界上军事实力最强大的国家"
}

PUT t1/doc/3
{
  "title": "北京是中国的首都"
}
```

我们想要查询和中国有关的文档，按照正常的 match 条件查询方式去查询

```python
GET t1/doc/_search
{
  "query": {
    "match": {
      "title": "中国"
    }
  }
}
```

查询结果：

```python
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.68324494,
    "hits" : [
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "1",
        "_score" : 0.68324494,
        "_source" : {
          "title" : "中国是世界上人口最多的国家"
        }
      },
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "3",
        "_score" : 0.5753642,
        "_source" : {
          "title" : "北京是中国的首都"
        }
      },
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "2",
        "_score" : 0.39556286,
        "_source" : {
          "title" : "美国是世界上军事实力最强大的国家"
        }
      }
    ]
  }
}
```

发现虽然包含中国的数据都拿到了，但是还是出现了我们不想要的数据，比如关于美国的数据，这其实就是我们常说的脏数据。那么为什么我查询的是中国，ES 也会把美国相关的数据返回呢？这就要简单了解一下 ES 的分词机制了。ES 在内部对文档做分词的时候，对于中文来说，就是一个字一个字分的，所以，我们搜"中国"，"中"和"国"都符合条件，而美国的"国"也符合。而我们认为"中国"是个短语，是一个有具体含义的词，但是 ES 无法做到这么智能，所以再不借助分词插件的前提下， ES 在处理中文分词方面比较弱。我们可以通过以下方式来验证一下：

```python
# 以ES自带的标准分词规则来对"中国"进行分词
GET _analyze
{
  "text":"中国",
  "analyzer": "standard"
}
```

分词结果：

```python
{
  "tokens" : [
    {
      "token" : "中",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "国",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    }
  ]
}
```

我们后面会研究针对中文的分词插件，但目前我们还有其他办法解决，那就是使用短语查询：

```python
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": "中国"
    }
  }
}
```

查询结果

```python
{
  "took" : 15,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.5753642,
    "hits" : [
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "1",
        "_score" : 0.5753642,
        "_source" : {
          "title" : "中国是世界上人口最多的国家"
        }
      },
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "3",
        "_score" : 0.5753642,
        "_source" : {
          "title" : "北京是中国的首都"
        }
      }
    ]
  }
}
```

这里短语查询是在文档中搜索指定的词组，而"中国"正是一个词组，所以就可以只查询和中国有关的文档了。那么如果我想查询包含"中国"和"世界"关键字的数据呢？我们还是尝试使用短语查询

```python
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": "中国世界"
    }
  }
}
```

查询结果

```python
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

没有查询到任何的数据，这是因为并没有"中国世界"连在一起的词组存在，我们构造的数据中这两个词语中间还夹杂着其他的汉字。此时我们可以使用 slop 参数，它可以允许词组之间出现间隔，相当于正则表达式中的 "中国.*?世界"，不去人为指定值的话，slop 默认为 0

```python
GET t1/doc/_search
{
  "query": {
    "match_phrase": {
      "title": {
        "query": "中国世界",
        "slop": 1
      }
    }
  }
}
```

查询结果

```python
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.7445889,
    "hits" : [
      {
        "_index" : "t1",
        "_type" : "doc",
        "_id" : "1",
        "_score" : 0.7445889,
        "_source" : {
          "title" : "中国是世界上人口最多的国家"
        }
      }
    ]
  }
}
```

这里 slop 设置为1，相当于是把文档中 "中国是世界" 的 "是" 当作一个间隔了。slop 的值一定要大于等于词组间的间隔数。

#### **1.4 match_phrase_prefix 前缀查询**

前缀查询与短语查询类似，但前缀查询可以更进一步的搜索词组，只不过它是和词组中的某个词条进行前缀匹配。我们先来造一些数据：

```python
PUT s1/doc/1
{
  "title": "maggie",
  "desc": "beautiful girl you are beautiful so"
}


PUT s1/doc/2
{
  "title": "sun and beach",
  "desc": "I like basking on the beach"
}
```

此时我们想查询含有 "bea" 开头的单词的数据，显然用普通的查询和短语查询是无法实现的，可以使用前缀查询

```python
GET s1/doc/_search
{
  "query": {
    "match_phrase_prefix": {
      "desc": "bea"
    }
  }
}
```

查询结果

```python
{
  "took" : 17,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.39556286,
    "hits" : [
      {
        "_index" : "s1",
        "_type" : "doc",
        "_id" : "1",
        "_score" : 0.39556286,
        "_source" : {
          "title" : "maggie",
          "desc" : "beautiful girl you are beautiful so"
        }
      },
      {
        "_index" : "s1",
        "_type" : "doc",
        "_id" : "2",
        "_score" : 0.2876821,
        "_source" : {
          "title" : "sun and beach",
          "desc" : "I like basking on the beach"
        }
      }
    ]
  }
}
```

该查询方式的应用也非常的广泛，比如搜索框的提示信息等等。当使用这种方式进行搜索时，最好通过 "max_expansions" 来设置最大的前缀扩展数量，因为产生的结果会是一个很大的集合，不加限制的话，可能会影响查询性能。"max_expansions" 默认值是50

```python
GET s1/doc/_search
{
  "query": {
    "match_phrase_prefix": {
      "desc":{
        "query": "bea",
        "max_expansions": 1
      }
    }
  }
}
```

但是，如果此时你去尝试加上 "max_expansions" 并进行测试，你会发现并没有如你想想的一样，仅返回一条数据，而是返回了多条数据。"max_expansions" 执行的是搜索的编辑（Levenshtein）距离。那什么是编辑距离呢？编辑距离是一种计算两个字符串间的差异程度的字符串度量（string metric）。我们可以认为编辑距离就是从一个字符串修改到另一个字符串时，其中编辑单个字符（比如修改、插入、删除）所需要的最少次数（俄罗斯科学家Vladimir Levenshtein于1965年提出了这一概念）。

​		我们再引用elasticsearch官网的一段话：**该max_expansions设置定义了在停止搜索之前模糊查询将匹配的最大术语数，也可以对模糊查询的性能产生显着影响。但是，减少查询字词会产生负面影响，因为查询提前终止可能无法找到某些有效结果。重要的是要理解max_expansions查询限制在分片级别工作，这意味着即使设置为1，多个术语可能匹配，所有术语都来自不同的分片。此行为可能使其看起来好像max_expansions没有生效，因此请注意，计算返回的唯一术语不是确定是否有效的有效方法max_expansions。**我们只需知道该参数工作于分片层，也就是Lucene部分；使用前缀查询会非常的影响性能，要对结果集进行限制，就加上这个参数。

#### **1.5 multi_match 多字段查询**

```python
PUT s1/doc/3
{
  "title": "maggie is beautiful girl",
  "desc": "beautiful girl you are beautiful so"
}

PUT s1/doc/4
{
  "title": "beautiful beach",
  "desc": "I like basking on the beach,and you? beautiful girl"
}
```

multi_match 可用于多字段查询，例如查询 "title" 和 "desc" 中同时含有 "beautiful" 的数据

```python
GET s1/doc/_search
{
  "query": {
    "multi_match": {
      "query": "beautiful",
      "fields": ["title", "desc"]
    }
  }
}
```

除此之外，"multi_match" 甚至可以和 "match_phrase" 、"match_phrase_prefix" 结合使用，只需要指定 "type" 类型即可：

```python
GET s1/doc/_search
{
  "query": {
    "multi_match": {
      "query": "gi",
      "fields": ["title","desc"],
      "type": "phrase_prefix"
    }
  }
}
GET t3/doc/_search
{
  "query": {
    "multi_match": {
      "query": "girl",
      "fields": ["title"],
      "type": "phrase"
    }
  }
}
```

### **二、term 查询**

​		在介绍 term 查询之前我们有必要先了解一下 ES 的分词机制。默认情况下，elasticsearch 在对文档分析期间（将文档分词后保存到倒排索引的过程），会对文档进行分词，比如默认的标准分析器会按照如下步骤对文档进行分析：

- 删除大多数的标点符号
- 将文档分解为单个词条，我们称为token
- 将token转为小写

例如 "Beautiful girl!"，在经过分析后是这样的了：

```python
POST _analyze
{
  "analyzer": "standard",
  "text": "Beautiful girl!"
}

# 结果
["beautiful", "girl"]
```

```python
PUT w10
{
  "mappings": {
    "doc":{
      "properties":{
        "t1":{
          "type": "text"
        }
      }
    }
  }
}

PUT w10/doc/1
{
  "t1": "Beautiful girl!"
}

PUT w10/doc/2
{
  "t1": "sexy girl!"
}

GET w10/doc/_search
{
  "query": {
    "match": {
      "t1": "Beautiful girl!"
    }
  }
}
```

当在使用match查询时，elasticsearch 会使用同样的分析方式对查询关键字进行分析。也就是对查询关键字 "Beautiful girl!" 进行分析，得到 ["beautiful", "girl"]，然后分别将这两个单独的token去索引w10 中进行查询，结果就是将两篇文档都返回。因为文档中至少都包含了某一个检索的关键字。这在有些情况下是非常好用的，但是，如果我们想精确查询，将 "Beautiful girl!" 当成一个token而不是分词后的两个token。这就要用到了term查询了，**term查询的是没有经过分析的查询关键字**。

但是，如果你要查询的字段类型是 text 类型的，那么 term 查询就不太适用了，因为 term 是查询没有经过分析的关键字，比如：

```python
GET w10/doc/_search
{
  "query": {
    "term": {
      "t1": "Beautiful girl!"
    }
  }
}
```

上述查询不会有任何结果，因为在插入数据时，"Beautiful girl!" 已经经过分析成了关键检索词 ["beautiful", "girl"]，而 term 又是只查询没有经过分析的关键字，换句话说，数据已经变成了 ["beautiful", "girl"]，而你还想用 "Beautiful girl!"  去匹配，显然是匹配不到的。

```python
GET w10/doc/_search
{
  "query": {
    "term": {
      "t1": "Beautiful"
    }
  }
}
```

上述查询也没有结果，原因也是一样， ["beautiful", "girl"] 中找不到 "Beautiful"。所以一般 term 都是和 keyword 类型数据查询联合使用，因为 elasticsearch 会将 keyword 类型的字段当成一个 token 保存到倒排索引上，而不会对其进行分析。

```python
PUT w10
{
  "mappings": {
    "doc":{
      "properties":{
        "t1":{
          "type": "keyword"
        }
      }
    }
  }
}

PUT w10/doc/1
{
  "t1": "Beautiful girl!"
}

# 查询
GET w10/doc/_search
{
  "query": {
    "term": {
      "t1": "Beautiful girl!"
    }
  }
}
```

同时，term 还支持多个关键字的查询：

```python
PUT w10
{
  "mappings": {
    "doc":{
      "properties":{
        "t1":{
          "type": "text"
        }
      }
    }
  }
}

PUT w10/doc/1
{
  "t1": "Beautiful girl!"
}

PUT w10/doc/2
{
  "t1": "sexy girl!"
}

# 这里term查询的小写处理后的beautiful，所以可以查到数据
GET w10/doc/_search
{
  "query": {
    "terms": {
      "t1": ["beautiful", "sexy"]
    }
  }
}
```